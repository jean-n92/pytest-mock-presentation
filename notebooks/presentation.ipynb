{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## How to make your testing life easier by using unittest.**mock** package\n",
    "\n",
    "### A small introduction to mocking, APIs and cats"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is mocking?"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ">`unittest.mock` is a library for testing in Python. It allows you to replace parts of your system under test with mock objects and make assertions about how they have been used."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![SO](http://localhost:5500/lib/img3.png)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Hidden imports go here\n",
    "import sys\n",
    "import ipytest\n",
    "import requests\n",
    "import pyspark\n",
    "import unittest\n",
    "from unittest import mock"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with the *very* basics:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def a_random_function(arg1: str, arg2: str) -> str:\n",
    "    return arg2 + arg1\n",
    "\n",
    "a_random_function(\"AMRO\", \"ABN\")"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's now make things slightly less linear:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def another_function(arg1: int, arg2: int, arg3: str, arg4: str) -> str:\n",
    "    return a_random_function(arg3, arg4) + str((arg1 + arg2)//2)\n",
    "\n",
    "another_function(2, 4, \" ò=)‚à´\", \"(= ò·ÜΩ\")"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How is this related to unit testing?    \n",
    "Let's take the definition of *unit testing*. According to Wikipedia (the main source of information for our century), we can define unit testing as:\n",
    ">\"A software testing method by which individual units of source code ‚Äî sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures ‚Äî are tested to determine whether they are fit for use.\""
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dilemma is:\n",
    "> \" How can we test something over which we have no control? \""
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our specific case, let's take `a_random_function(arg1, arg2)` as given.    \n",
    "Let's say we import it from some other package, that doesn't belong to us.    "
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We run into this issue many times: the most common time is when we interact with the filesystem."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's redefine our `another_function()` as follows:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def another_function(arg1: str, arg2: str) -> str:\n",
    "    return a_random_function(sys.argv[0], sys.argv[1]) + \" - \" + str((arg1 + arg2)//2)\n",
    "\n",
    "another_function(10, 200)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, I have no possible control over `sys.argv`: they get defined at runtime, once I run my application.    \n",
    "How can we account for them, or get *some* degree of control?"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fortunately, `unittest.mock` allows us to perform this task in a relatively relaxed way."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@mock.patch(\"sys.argv\", [\"One\", \"Two\"])\n",
    "def another_function(arg1: str, arg2: str) -> str:\n",
    "    return a_random_function(sys.argv[0], sys.argv[1]) + \" - \" + str((arg1 + arg2)//2)\n",
    "\n",
    "another_function(10, 200)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sys.argv)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![cat](http://localhost:5500/lib/img1.jpg)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test-related imports go here\n",
    "\n",
    "ipytest.autoconfig()\n",
    "test_args = [\"--showlocals\", \n",
    "            \"-x\", \n",
    "            \"--cov-report\", \n",
    "            \"term-missing\",\n",
    "            \"--cov\",\n",
    "            \"neon.functions\"]"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's get to our cats üêà\n",
    "Since I really love cats, I'd like to know more about them.    \n",
    "Fortunately, someone created the **completely free** [Cat facts API](https://catfact.ninja/) which can return nice (and interesting) facts about our feline friends."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "requests.get(url=\"https://catfact.ninja/fact\").json()[\"fact\"]"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of this presentation, I made a **whole application** to collect facts about cats. The application does two main things:\n",
    "- Collects a certain number of facts by querying the API\n",
    "- Saves everything into my local Spark cluster, into a well organized table   \n",
    "\n",
    "I called the application **Neon** (just because that's the first name I got from a random generator)    \n",
    "The application code and its tests are freely available on my github profile (link at the end of the presentation)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's start making use of it then!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from neon.functions import *"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![outline](http://localhost:5500/lib/img4.png)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "interesting_facts = process_data(usernumber=3)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Each API call generates a JSONs. `process_data()` conveniently packs them into a list, over which we can easily iterate: "
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for entry in interesting_facts:\n",
    "    print(entry[\"fact\"])"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now store these important facts inside my table, so then they don't get lost:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark = establish_spark()\n",
    "group_and_save(spark, facts=interesting_facts)\n",
    "df = spark.read.table(\"default.random_cats_facts\")"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.show(100)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the whole procedure is very slow. This is due to a random `waiting` variable that I added in order to simulate a slow connection, a very server side, or anything that can get in between.    \n",
    "Since the `waiting` variable polls a random amount of seconds between 1 and 10, we're talking here about an average waiting time of 5 seconds (if you don't believe me, check the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) for statistical reference).    "
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What can I do to make the whole thing more efficient, while testing for the main functionality?"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two main issues over here:\n",
    "1) I don't wanna wait 5 seconds for each call to the API, but I'd like to be sure that the call works nonetheless (i.e. no test data, I don't care about the data: I want my functionality to work)    \n",
    "2) I don't wanna save the data I retrieve every single time, since I have no control over it will be saved: I just know that, as soon as I run the application, the data will be saved somewhere. That's not good for testing purposes!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use our mocks:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mention again that this is our stuff\n",
    "class TestAPI(unittest.TestCase):\n",
    "    @mock.patch(\"neon.functions.make_request\", return_value=None)\n",
    "    def test_retrieve_data_without_waiting(self, patched_request):\n",
    "        t0 = time.perf_counter()\n",
    "        actual: dict = retrieve_data(waiting=5)\n",
    "        actual_keys: list = [key for key in actual]\n",
    "        expected_keys: list = [\"fact\", \"length\"]\n",
    "        t1 = time.perf_counter() - t0\n",
    "        self.assertEqual(actual_keys, expected_keys)\n",
    "        self.assertLess(t1, 10)\n",
    "    \n",
    "    @mock.patch(\"neon.functions.retrieve_data\", return_value={\"fact\" : \"This is a random fact\", \"length\" : \"-99\"})\n",
    "    def test_process_data_with_custom_load(self, patched_retrieve):\n",
    "        t0 = time.perf_counter()\n",
    "        actual: list = process_data(usernumber=5, waiting=1)\n",
    "        expected: list = [{\"fact\" : \"This is a random fact\", \"length\" : \"-99\"}]*5\n",
    "        t1 = time.perf_counter() - t0\n",
    "        self.assertEqual(actual, expected)\n",
    "        self.assertLess(t1, 10)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "first_test_suite = TestAPI()\n",
    "ipytest.run(*test_args)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestSparkFunctions(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.mocked_data: List[dict] = [{\"fact\": \"This is a random fact\", \"length\": \"-99\"}]\n",
    "\n",
    "    @mock.patch(\"pyspark.sql.readwriter.DataFrameWriter.saveAsTable\")\n",
    "    def test_group_and_save_with_patching(self, patched_writer):\n",
    "        patched_writer.new = True\n",
    "        spark: SparkSession = establish_spark()\n",
    "        data: List[dict] = self.mocked_data\n",
    "        group_and_save(spark, data)\n",
    "        patched_writer.assert_called()\n",
    "\n",
    "    @mock.patch(\"neon.functions.make_request\", return_value=None)\n",
    "    def test_group_and_save_with_api_load(self, patched_request):\n",
    "        mocked_save = mock.create_autospec(group_and_save)\n",
    "        spark: SparkSession = establish_spark()\n",
    "        data: list[dict] = process_data(usernumber=5, waiting=2)\n",
    "        expected: bool = mocked_save(spark, data)\n",
    "        self.assertTrue(expected)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "second_test_suite = TestSparkFunctions()\n",
    "ipytest.run(*test_args)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem solved!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![cat](http://localhost:5500/lib/img2.jpg)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful links:\n",
    "[Catfacts (API reference)](https://catfact.ninja/)    \n",
    "[This application (Git repo)](https://github.com/jean-n92/pytest-mock-presentation)    \n",
    "[Where to patch (Documentation)](https://docs.python.org/3/library/unittest.mock.html#where-to-patch)    \n",
    "[Quick start with mock (Documentation)](https://docs.python.org/3/library/unittest.mock.html#quick-guide)\n",
    "[Difference between mock and stub (StackOverflow)](https://stackoverflow.com/questions/3459287/whats-the-difference-between-a-mock-stub)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "4a7b606e264437b67c76b6a90ca9c891af4f0d4e775e00cab262968ebd59eaa3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('venv': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}