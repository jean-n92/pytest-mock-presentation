{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## How to make your testing life easier by using unittest.**mock** package\n",
    "\n",
    "### A small introduction to mocking, APIs and cats"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is mocking?"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ">`unittest.mock` is a library for testing in Python. It allows you to replace parts of your system under test with mock objects and make assertions about how they have been used."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![SO](http://localhost:5500/lib/img3.png)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Hidden imports go here\n",
    "import sys\n",
    "import ipytest\n",
    "import requests\n",
    "import pyspark\n",
    "import unittest\n",
    "from unittest import mock"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with the *very* basics:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def a_random_function(arg1: str, arg2: str) -> str:\n",
    "    return arg2 + arg1\n",
    "\n",
    "a_random_function(\"AMRO\", \"ABN\")"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's now make things slightly less linear:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def another_function(arg1: int, arg2: int, arg3: str, arg4: str) -> str:\n",
    "    return a_random_function(arg3, arg4) + str((arg1 + arg2)//2)\n",
    "\n",
    "another_function(2, 4, \"Ê˜=)âˆ«\", \"(=Ê˜á†½\")"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How is this related to unit testing?    \n",
    "Let's take the definition of *unit testing*. According to Wikipedia (the main source of information for our century), we can define unit testing as:\n",
    ">\"A software testing method by which individual units of source code â€” sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures â€” are tested to determine whether they are fit for use.\""
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dilemma is:\n",
    "> \" How can we test something over which we have no control? \""
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our specific case, let's take `a_random_function(arg1, arg2)` as given.    \n",
    "Let's say we import it from some other package, that doesn't belong to us.    "
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We run into this issue many times: the most common time is when we interact with the filesystem."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's redefine our `another_function()` as follows:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def another_function(arg1: str, arg2: str) -> str:\n",
    "    return a_random_function(sys.argv[0], sys.argv[1]) + \" - \" + str((arg1 + arg2)//2)\n",
    "\n",
    "another_function(10, 200)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, I have no possible control over `sys.argv`: they get defined at runtime, once I run my application.    \n",
    "How can we account for them, or get *some* degree of control?"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fortunately, `unittest.mock` allows us to perform this task in a relatively relaxed way."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@mock.patch(\"sys.argv\", [\"One\", \"Two\"])\n",
    "def another_function(arg1: str, arg2: str) -> str:\n",
    "    return a_random_function(sys.argv[0], sys.argv[1]) + \" - \" + str((arg1 + arg2)//2)\n",
    "\n",
    "another_function(10, 200)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sys.argv)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![cat](http://localhost:5500/lib/img1.jpg)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test-related imports go here\n",
    "\n",
    "ipytest.autoconfig()\n",
    "test_args = [\"--showlocals\", \n",
    "            \"-x\", \n",
    "            \"--cov-report\", \n",
    "            \"term-missing\",\n",
    "            \"--cov\",\n",
    "            \"neon.functions\"]"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's get to our cats ðŸˆ\n",
    "Since I really love cats, I'd like to know more about them.    \n",
    "Fortunately, someone created the **completely free** [Cat facts API](https://catfact.ninja/) which can return nice (and interesting) facts about our feline friends."
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "requests.get(url=\"https://catfact.ninja/fact\").json()[\"fact\"]"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of this presentation, I made a **whole application** to collect facts about cats. The application does two main things:\n",
    "- Collects a certain number of facts by querying the API\n",
    "- Saves everything into my local Spark cluster, into a well organized table   \n",
    "\n",
    "I called the application **Neon** (just because that's the first name I got from a random generator)    \n",
    "The application code and its tests are freely available on my github profile (link at the end of the presentation)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's start making use of it then!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from neon.functions import *"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![outline](http://localhost:5500/lib/img4.png)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "interesting_facts = process_data(usernumber=3)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Each API call generates a JSONs. `process_data()` conveniently packs them into a list, over which we can easily iterate: "
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for entry in interesting_facts:\n",
    "    print(entry[\"fact\"])"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now store these important facts inside my table, so then they don't get lost:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark = establish_spark()\n",
    "group_and_save(spark, facts=interesting_facts)\n",
    "df = spark.read.table(\"default.random_cats_facts\")"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.show(100)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the whole procedure is very slow. This is due to a random `waiting` variable that I added in order to simulate a slow connection, a very server side, or anything that can get in between.    \n",
    "Since the `waiting` variable polls a random amount of seconds between 1 and 10, we're talking here about an average waiting time of 5 seconds (if you don't believe me, check the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) for statistical reference).    "
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What can I do to make the whole thing more efficient, while testing for the main functionality?"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two main issues over here:\n",
    "1) I don't wanna wait 5 seconds for each call to the API, but I'd like to be sure that the call works nonetheless (i.e. no test data, I don't care about the data: I want my functionality to work)    \n",
    "2) I don't wanna save the data I retrieve every single time, since I have no control over it will be saved: I just know that, as soon as I run the application, the data will be saved somewhere. That's not good for testing purposes!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use our mocks:"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mention again that this is our stuff\n",
    "class TestAPI(unittest.TestCase):\n",
    "    @mock.patch(\"neon.functions.make_request\", return_value=None)\n",
    "    def test_retrieve_data_without_waiting(self, patched_request):\n",
    "        t0 = time.perf_counter()\n",
    "        actual: dict = retrieve_data(waiting=5)\n",
    "        actual_keys: list = [key for key in actual]\n",
    "        expected_keys: list = [\"fact\", \"length\"]\n",
    "        t1 = time.perf_counter() - t0\n",
    "        self.assertEqual(actual_keys, expected_keys)\n",
    "        self.assertLess(t1, 10)\n",
    "    \n",
    "    @mock.patch(\"neon.functions.retrieve_data\", return_value={\"fact\" : \"This is a random fact\", \"length\" : \"-99\"})\n",
    "    def test_process_data_with_custom_load(self, patched_retrieve):\n",
    "        t0 = time.perf_counter()\n",
    "        actual: list = process_data(usernumber=5, waiting=1)\n",
    "        expected: list = [{\"fact\" : \"This is a random fact\", \"length\" : \"-99\"}]*5\n",
    "        t1 = time.perf_counter() - t0\n",
    "        self.assertEqual(actual, expected)\n",
    "        self.assertLess(t1, 10)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "first_test_suite = TestAPI()\n",
    "ipytest.run(*test_args)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestSparkFunctions(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.mocked_data: List[dict] = [{\"fact\": \"This is a random fact\", \"length\": \"-99\"}]\n",
    "\n",
    "    @mock.patch(\"pyspark.sql.readwriter.DataFrameWriter.saveAsTable\")\n",
    "    def test_group_and_save_with_patching(self, patched_writer):\n",
    "        patched_writer.new = True\n",
    "        spark: SparkSession = establish_spark()\n",
    "        data: List[dict] = self.mocked_data\n",
    "        group_and_save(spark, data)\n",
    "        patched_writer.assert_called()\n",
    "\n",
    "    @mock.patch(\"neon.functions.make_request\", return_value=None)\n",
    "    def test_group_and_save_with_api_load(self, patched_request):\n",
    "        mocked_save = mock.create_autospec(group_and_save)\n",
    "        spark: SparkSession = establish_spark()\n",
    "        data: list[dict] = process_data(usernumber=5, waiting=2)\n",
    "        expected: bool = mocked_save(spark, data)\n",
    "        self.assertTrue(expected)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "second_test_suite = TestSparkFunctions()\n",
    "ipytest.run(*test_args)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem solved!"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![cat](http://localhost:5500/lib/img2.jpg)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful links:\n",
    "[Catfacts (API reference)](https://catfact.ninja/)    \n",
    "[This application (Git repo)](https://github.com/jean-n92/pytest-mock-presentation)    \n",
    "[Where to patch (Documentation)](https://docs.python.org/3/library/unittest.mock.html#where-to-patch)    \n",
    "[Quick start with mock (Documentation)](https://docs.python.org/3/library/unittest.mock.html#quick-guide)\n",
    "[Difference between mock and stub (StackOverflow)](https://stackoverflow.com/questions/3459287/whats-the-difference-between-a-mock-stub)"
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "4a7b606e264437b67c76b6a90ca9c891af4f0d4e775e00cab262968ebd59eaa3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('venv': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}